{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "1. Make a copy of this notebook (EasyOCR.ipynb)\n",
        "  * Upload the EasyOCR.ipynb to Google Colab\n",
        "  * Click on \"Copy to Drive\"\n",
        "\n",
        "2. Adjust input and output folders (IMAGE_DIR, OUTPUT_DIR)\n",
        "  * Under the block \"Manage Input and Output\", change the following directories:\n",
        "    *   IMAGE_DIR should be the path to a folder of your input images\n",
        "    *   OUTPUT_DIR should be the desired folder location for the output txt files\n",
        "\n",
        "3. Run the OCR Script\n",
        "  * Click on \"Runtime\"\n",
        "  * Click \"Run all\" to run all the cells\n",
        "\n"
      ],
      "metadata": {
        "id": "z-nCb0agTW1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "f1qJLV3YThfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr"
      ],
      "metadata": {
        "id": "ebkmniWUEaDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manage Input and Output"
      ],
      "metadata": {
        "id": "KtI1Ki2d2fzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eLQwkOjn22ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = '/content/drive/My Drive/OCR/Input_OCR_Textures/'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/OCR/Output_OCR_Files/'"
      ],
      "metadata": {
        "id": "gAeRnnro2lJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optical Character Recognition on Engraved Headstones"
      ],
      "metadata": {
        "id": "ksiAQm8dTlt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import easyocr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import requests\n",
        "from difflib import get_close_matches\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def preprocess_image(image):\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray_equalized = cv2.equalizeHist(gray)\n",
        "    gamma_corrected = cv2.pow(gray_equalized / 255.0, 1) #gamme\n",
        "    gamma_corrected = (gamma_corrected * 255).astype(np.uint8)\n",
        "    toned_down_img = cv2.convertScaleAbs(gamma_corrected, alpha=1.8, beta=0) #alpha, beta\n",
        "\n",
        "    thresh = cv2.adaptiveThreshold(toned_down_img, 245, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n",
        "                                        cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    return thresh\n",
        "\n",
        "def download_english_words():\n",
        "    url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\n",
        "    response = requests.get(url)\n",
        "    english_words = set(response.text.split())\n",
        "\n",
        "    return english_words\n",
        "\n",
        "def extract_text(image, image_path):\n",
        "\n",
        "    preprocessed_img = preprocess_image(image)\n",
        "\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    result = reader.readtext(preprocessed_img)\n",
        "\n",
        "    english_words = download_english_words()\n",
        "\n",
        "    # Specify structure shape and kernel size\n",
        "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n",
        "\n",
        "    # Applying dilation on threshhold image\n",
        "    dilation = cv2.dilate(preprocessed_img, rect_kernel, iterations = 1)\n",
        "\n",
        "    # Finding contours\n",
        "    contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "    # Create a copy of the image\n",
        "    im2 = image.copy()\n",
        "\n",
        "    # Display bounding boxes around detected text\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(cv2.cvtColor(preprocessed_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    for contour in contours:\n",
        "      # Bounding box detection (using contour detection)\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "      rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "\n",
        "      # Cropping the text box\n",
        "      cropped = im2[y:y + h, x:x + w]\n",
        "\n",
        "      # Run EasyOCR on cropped text\n",
        "      result = reader.readtext(cropped)\n",
        "\n",
        "      extracted_text = []\n",
        "      for detection in result:\n",
        "         # Bounding box coordinates (using text detection)\n",
        "          points = detection[0]\n",
        "          x, y, x1, y1 = points[0][0], points[0][1], points[2][0], points[2][1]\n",
        "          w, h = x1 - x, y1 - y\n",
        "\n",
        "          rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "          ax.add_patch(rect)\n",
        "\n",
        "          # Find the closest English word\n",
        "          detected_text = detection[1].lower()\n",
        "          detected_text = detected_text.replace(\"[\", \"1\").replace(\"(\", \"1\")\n",
        "          closest_word = get_close_matches(detected_text, english_words, n=1)[0] if detected_text in english_words else None\n",
        "\n",
        "          extracted_text.append(closest_word.upper() if closest_word else detected_text.upper())\n",
        "          # print(detection[1].upper())\n",
        "\n",
        "    ax.set_title(os.path.basename(image_path))\n",
        "    plt.show()\n",
        "    return extracted_text if extracted_text else []\n",
        "\n",
        "def process_images_in_folder(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_path = os.path.join(input_folder, filename)\n",
        "            image = cv2.imread(image_path)\n",
        "            extracted_text = extract_text(image, filename)\n",
        "\n",
        "            output_file_path = os.path.join(output_folder, os.path.splitext(filename)[0] + \".txt\")\n",
        "            with open(output_file_path, \"w\") as txt_file:\n",
        "                for line in extracted_text:\n",
        "                    txt_file.write(line + \"\\n\")\n",
        "\n",
        "            print(f\"Output produced for image: {filename}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_images_in_folder(IMAGE_DIR, OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "LGJkMz-zChiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "f1qJLV3YThfy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}